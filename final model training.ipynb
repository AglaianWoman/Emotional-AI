{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras.preprocessing import image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "#Using Kaggle FER2013 data set\n",
    "df = pd.read_excel('data/fer2013.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from math import ceil \n",
    "\n",
    "indices = list(range(len(df[df['Usage'] == 'Training'])))\n",
    "np.random.shuffle(indices)\n",
    "train_indices = indices[ceil(len(indices)/5):]\n",
    "valid_indices = indices[:ceil(len(indices)/5)]\n",
    "labels = df.emotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "one_hot_labels = keras.utils.to_categorical(df.emotion, num_classes=len(df.emotion.unique()))\n",
    "test_labels = one_hot_labels[~df.index.isin(indices)]\n",
    "train_labels = one_hot_labels[train_indices]\n",
    "valid_labels = one_hot_labels[valid_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def to_rgb1a(im):\n",
    "    # Convert to RGB\n",
    "    w, h = im.shape\n",
    "    ret = np.empty((w, h, 3), dtype=np.float16)\n",
    "    ret[:, :, 2] =  ret[:, :, 1] =  ret[:, :, 0] =  im\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2min 6s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df['pixels'] = df['pixels'].apply(lambda x: to_rgb1a(np.reshape(np.array(x.split()),(48,48))))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def paths_to_tensor(arrs):\n",
    "    list_of_tensors = [np.expand_dims(arr, axis = 0) for arr in arrs]\n",
    "    return np.vstack(list_of_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img_input = preprocess_input(paths_to_tensor(df['pixels']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "datagen =  ImageDataGenerator(\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_generator = datagen.flow(\n",
    "        x = img_input[train_indices],\n",
    "        y = train_labels,\n",
    "        batch_size=batch_size)    \n",
    "\n",
    "validation_generator = datagen.flow(\n",
    "        x = img_input[valid_indices],\n",
    "        y = valid_labels,\n",
    "        batch_size=batch_size)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 48, 48, 16)        208       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 48, 48, 16)        1040      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 24, 24, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 24, 24, 32)        2080      \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 24, 24, 32)        4128      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 12, 12, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 12, 12, 64)        8256      \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 12, 12, 64)        16448     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 6, 6, 64)          0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 6, 6, 128)         8320      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 6, 6, 256)         33024     \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_1 ( (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 7)                 1799      \n",
      "=================================================================\n",
      "Total params: 75,303.0\n",
      "Trainable params: 75,303.0\n",
      "Non-trainable params: 0.0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.layers import Dropout, Flatten, Dense\n",
    "from keras.models import Sequential\n",
    "\n",
    "model5 = Sequential()\n",
    "model5.add(Conv2D(filters=16, kernel_size=2,padding='same', activation='relu', input_shape=img_input[train_indices].shape[1:]))\n",
    "model5.add(Conv2D(filters=16, kernel_size=2, padding='same', activation='relu'))\n",
    "model5.add(MaxPooling2D(pool_size=2))\n",
    "model5.add(Conv2D(filters=32, kernel_size=2, padding='same', activation='relu'))\n",
    "model5.add(Conv2D(filters=32, kernel_size=2, padding='same', activation='relu'))\n",
    "model5.add(MaxPooling2D(pool_size=2))\n",
    "model5.add(Conv2D(filters=64, kernel_size=2, padding='same', activation='relu'))\n",
    "model5.add(Conv2D(filters=64, kernel_size=2, padding='same', activation='relu'))\n",
    "model5.add(MaxPooling2D(pool_size=2))\n",
    "model5.add(Dense(128, activation='relu'))\n",
    "model5.add(Dense(256, activation='relu'))\n",
    "model5.add(GlobalAveragePooling2D())\n",
    "#model5.add(Dropout(0.5))\n",
    "model5.add(Dense(7, activation='softmax'))\n",
    "model5.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "717/717 [============================>.] - ETA: 0s - loss: 1.7931 - acc: 0.2562Epoch 00000: val_loss improved from inf to 1.77708, saving model to face.weights.best.model5c2-00-1.78.hdf5\n",
      "718/717 [==============================] - 115s - loss: 1.7933 - acc: 0.2561 - val_loss: 1.7771 - val_acc: 0.2612\n",
      "Epoch 2/30\n",
      "717/717 [============================>.] - ETA: 0s - loss: 1.7214 - acc: 0.2940Epoch 00001: val_loss improved from 1.77708 to 1.71872, saving model to face.weights.best.model5c2-01-1.72.hdf5\n",
      "718/717 [==============================] - 114s - loss: 1.7211 - acc: 0.2941 - val_loss: 1.7187 - val_acc: 0.3032\n",
      "Epoch 3/30\n",
      "717/717 [============================>.] - ETA: 0s - loss: 1.6708 - acc: 0.3187Epoch 00002: val_loss improved from 1.71872 to 1.65961, saving model to face.weights.best.model5c2-02-1.66.hdf5\n",
      "718/717 [==============================] - 114s - loss: 1.6708 - acc: 0.3188 - val_loss: 1.6596 - val_acc: 0.3361\n",
      "Epoch 4/30\n",
      "717/717 [============================>.] - ETA: 0s - loss: 1.5939 - acc: 0.3629Epoch 00003: val_loss improved from 1.65961 to 1.58383, saving model to face.weights.best.model5c2-03-1.58.hdf5\n",
      "718/717 [==============================] - 114s - loss: 1.5933 - acc: 0.3631 - val_loss: 1.5838 - val_acc: 0.3689\n",
      "Epoch 5/30\n",
      "717/717 [============================>.] - ETA: 0s - loss: 1.5350 - acc: 0.3991Epoch 00004: val_loss improved from 1.58383 to 1.51454, saving model to face.weights.best.model5c2-04-1.51.hdf5\n",
      "718/717 [==============================] - 115s - loss: 1.5351 - acc: 0.3992 - val_loss: 1.5145 - val_acc: 0.4188\n",
      "Epoch 6/30\n",
      "717/717 [============================>.] - ETA: 0s - loss: 1.4837 - acc: 0.4253Epoch 00005: val_loss improved from 1.51454 to 1.46718, saving model to face.weights.best.model5c2-05-1.47.hdf5\n",
      "718/717 [==============================] - 117s - loss: 1.4840 - acc: 0.4250 - val_loss: 1.4672 - val_acc: 0.4472\n",
      "Epoch 7/30\n",
      "717/717 [============================>.] - ETA: 0s - loss: 1.4408 - acc: 0.4417Epoch 00006: val_loss improved from 1.46718 to 1.40749, saving model to face.weights.best.model5c2-06-1.41.hdf5\n",
      "718/717 [==============================] - 117s - loss: 1.4407 - acc: 0.4417 - val_loss: 1.4075 - val_acc: 0.4537\n",
      "Epoch 8/30\n",
      "717/717 [============================>.] - ETA: 0s - loss: 1.4075 - acc: 0.4574Epoch 00007: val_loss improved from 1.40749 to 1.36866, saving model to face.weights.best.model5c2-07-1.37.hdf5\n",
      "718/717 [==============================] - 116s - loss: 1.4074 - acc: 0.4576 - val_loss: 1.3687 - val_acc: 0.4767\n",
      "Epoch 9/30\n",
      "717/717 [============================>.] - ETA: 0s - loss: 1.3737 - acc: 0.4736Epoch 00008: val_loss did not improve\n",
      "718/717 [==============================] - 117s - loss: 1.3740 - acc: 0.4736 - val_loss: 1.3750 - val_acc: 0.4821\n",
      "Epoch 10/30\n",
      "717/717 [============================>.] - ETA: 0s - loss: 1.3470 - acc: 0.4800Epoch 00009: val_loss improved from 1.36866 to 1.32198, saving model to face.weights.best.model5c2-09-1.32.hdf5\n",
      "718/717 [==============================] - 115s - loss: 1.3473 - acc: 0.4800 - val_loss: 1.3220 - val_acc: 0.4984\n",
      "Epoch 11/30\n",
      "717/717 [============================>.] - ETA: 0s - loss: 1.3328 - acc: 0.4871Epoch 00010: val_loss improved from 1.32198 to 1.30256, saving model to face.weights.best.model5c2-10-1.30.hdf5\n",
      "718/717 [==============================] - 115s - loss: 1.3331 - acc: 0.4870 - val_loss: 1.3026 - val_acc: 0.5059\n",
      "Epoch 12/30\n",
      "717/717 [============================>.] - ETA: 0s - loss: 1.3090 - acc: 0.4972Epoch 00011: val_loss improved from 1.30256 to 1.28246, saving model to face.weights.best.model5c2-11-1.28.hdf5\n",
      "718/717 [==============================] - 116s - loss: 1.3088 - acc: 0.4973 - val_loss: 1.2825 - val_acc: 0.5158\n",
      "Epoch 13/30\n",
      "717/717 [============================>.] - ETA: 0s - loss: 1.2944 - acc: 0.5073Epoch 00012: val_loss improved from 1.28246 to 1.28115, saving model to face.weights.best.model5c2-12-1.28.hdf5\n",
      "718/717 [==============================] - 116s - loss: 1.2947 - acc: 0.5074 - val_loss: 1.2811 - val_acc: 0.5120\n",
      "Epoch 14/30\n",
      "717/717 [============================>.] - ETA: 0s - loss: 1.2840 - acc: 0.5090Epoch 00013: val_loss did not improve\n",
      "718/717 [==============================] - 116s - loss: 1.2842 - acc: 0.5089 - val_loss: 1.3032 - val_acc: 0.5129\n",
      "Epoch 15/30\n",
      "717/717 [============================>.] - ETA: 0s - loss: 1.2743 - acc: 0.5126Epoch 00014: val_loss did not improve\n",
      "718/717 [==============================] - 117s - loss: 1.2746 - acc: 0.5125 - val_loss: 1.2830 - val_acc: 0.5068\n",
      "Epoch 16/30\n",
      "717/717 [============================>.] - ETA: 0s - loss: 1.2672 - acc: 0.5148Epoch 00015: val_loss improved from 1.28115 to 1.24924, saving model to face.weights.best.model5c2-15-1.25.hdf5\n",
      "718/717 [==============================] - 117s - loss: 1.2671 - acc: 0.5148 - val_loss: 1.2492 - val_acc: 0.5336\n",
      "Epoch 17/30\n",
      "717/717 [============================>.] - ETA: 0s - loss: 1.2580 - acc: 0.5219Epoch 00016: val_loss did not improve\n",
      "718/717 [==============================] - 118s - loss: 1.2580 - acc: 0.5220 - val_loss: 1.3076 - val_acc: 0.5052\n",
      "Epoch 18/30\n",
      "717/717 [============================>.] - ETA: 0s - loss: 1.2462 - acc: 0.5258- ETA: 1s - loss: 1.2474 - aEpoch 00017: val_loss improved from 1.24924 to 1.23851, saving model to face.weights.best.model5c2-17-1.24.hdf5\n",
      "718/717 [==============================] - 80s - loss: 1.2462 - acc: 0.5257 - val_loss: 1.2385 - val_acc: 0.5272\n",
      "Epoch 19/30\n",
      "717/717 [============================>.] - ETA: 0s - loss: 1.2436 - acc: 0.5245 - ETA: 2s - Epoch 00018: val_loss did not improve\n",
      "718/717 [==============================] - 77s - loss: 1.2430 - acc: 0.5248 - val_loss: 1.2457 - val_acc: 0.5272\n",
      "Epoch 20/30\n",
      "717/717 [============================>.] - ETA: 0s - loss: 1.2361 - acc: 0.5289- ETA: 2s - lEpoch 00019: val_loss improved from 1.23851 to 1.21255, saving model to face.weights.best.model5c2-19-1.21.hdf5\n",
      "718/717 [==============================] - 77s - loss: 1.2359 - acc: 0.5289 - val_loss: 1.2125 - val_acc: 0.5472\n",
      "Epoch 21/30\n",
      "717/717 [============================>.] - ETA: 0s - loss: 1.2373 - acc: 0.5249Epoch 00020: val_loss did not improve\n",
      "718/717 [==============================] - 77s - loss: 1.2373 - acc: 0.5251 - val_loss: 1.2199 - val_acc: 0.5348\n",
      "Epoch 22/30\n",
      "717/717 [============================>.] - ETA: 0s - loss: 1.2274 - acc: 0.5320- ETA: 1s - loss: 1.2274 -Epoch 00021: val_loss did not improve\n",
      "718/717 [==============================] - 77s - loss: 1.2278 - acc: 0.5319 - val_loss: 1.2256 - val_acc: 0.5354\n",
      "Epoch 23/30\n",
      "717/717 [============================>.] - ETA: 0s - loss: 1.2167 - acc: 0.5319Epoch 00022: val_loss improved from 1.21255 to 1.20440, saving model to face.weights.best.model5c2-22-1.20.hdf5\n",
      "718/717 [==============================] - 77s - loss: 1.2166 - acc: 0.5318 - val_loss: 1.2044 - val_acc: 0.5428\n",
      "Epoch 24/30\n",
      "717/717 [============================>.] - ETA: 0s - loss: 1.2152 - acc: 0.5344Epoch 00023: val_loss did not improve\n",
      "718/717 [==============================] - 78s - loss: 1.2153 - acc: 0.5345 - val_loss: 1.2326 - val_acc: 0.5380\n",
      "Epoch 25/30\n",
      "717/717 [============================>.] - ETA: 0s - loss: 1.2152 - acc: 0.5363Epoch 00024: val_loss did not improve\n",
      "718/717 [==============================] - 77s - loss: 1.2148 - acc: 0.5365 - val_loss: 1.2432 - val_acc: 0.5374\n",
      "Epoch 26/30\n",
      "717/717 [============================>.] - ETA: 0s - loss: 1.2027 - acc: 0.5425Epoch 00025: val_loss did not improve\n",
      "718/717 [==============================] - 77s - loss: 1.2023 - acc: 0.5427 - val_loss: 1.2193 - val_acc: 0.5369\n",
      "Epoch 27/30\n",
      "717/717 [============================>.] - ETA: 0s - loss: 1.1974 - acc: 0.5487Epoch 00026: val_loss did not improve\n",
      "718/717 [==============================] - 77s - loss: 1.1973 - acc: 0.5486 - val_loss: 1.2277 - val_acc: 0.5350\n",
      "Epoch 28/30\n",
      "717/717 [============================>.] - ETA: 0s - loss: 1.2069 - acc: 0.5396- ETA: 1s - loss: 1.2066 - aEpoch 00027: val_loss improved from 1.20440 to 1.18723, saving model to face.weights.best.model5c2-27-1.19.hdf5\n",
      "718/717 [==============================] - 77s - loss: 1.2066 - acc: 0.5398 - val_loss: 1.1872 - val_acc: 0.5455\n",
      "Epoch 29/30\n",
      "717/717 [============================>.] - ETA: 0s - loss: 1.1869 - acc: 0.55060 ETA: 13 - ETA: 11s - loss: 1. - ETA: 10s - loss: 1.1886 - acc: 0.54 - ETA: 10s - loss: 1.18 - ETA: 1s - loss: 1.Epoch 00028: val_loss did not improve\n",
      "718/717 [==============================] - 77s - loss: 1.1870 - acc: 0.5507 - val_loss: 1.2256 - val_acc: 0.5357\n",
      "Epoch 30/30\n",
      "717/717 [============================>.] - ETA: 0s - loss: 1.1917 - acc: 0.5441Epoch 00029: val_loss did not improve\n",
      "718/717 [==============================] - 77s - loss: 1.1921 - acc: 0.5440 - val_loss: 1.1928 - val_acc: 0.5493"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "model5.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='face.weights.best.model5c2-{epoch:02d}-{val_loss:.2f}.hdf5', \n",
    "                               verbose=1, save_best_only=True)\n",
    "\n",
    "model5.fit_generator(train_generator,\n",
    "                    steps_per_epoch = len(train_indices)/batch_size,\n",
    "                    epochs=30,\n",
    "                    validation_data = validation_generator,\n",
    "                    validation_steps = len(valid_indices)/batch_size,\n",
    "                    callbacks = [checkpointer])\n",
    "\n",
    "model5.save_weights('best_weights_model5c2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "main_model = keras.models.load_model('C:/Users/Alvin/.keras/models/face.weights.best.model5c2-27-1.19.hdf5')\n",
    "main_model.load_weights('best_weights_model5c2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "score = model.evaluate(img_input[, y_test, batch_size=128)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
